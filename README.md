# KubeCon DRA Lab - Ansible Automation

Minimal, self-contained Ansible playbooks for setting up a Kubernetes 1.35 cluster with Dynamic Resource Allocation (DRA) support, simulated H100 GPUs, and K8s GPU resource management.

Perfect for learning DRA or testing GPU resource allocation without actual hardware.

## Architecture Overview

This setup creates:
- **2-node Kubernetes 1.35 cluster** (1 control plane + 1 worker)
- **CRI-O container runtime** (required for this DRA configuration)
- **fake-gpu-operator** - Simulates NVIDIA H100 GPUs via canhazgpu service
- **k8shazgpu** - DRA resource controller for GPU allocation
- **Test workloads** - Sample pods demonstrating DRA GPU claims

## Prerequisites

### On Your Workstation
- **Ansible** 2.15+ (`pip install ansible`)
- SSH access to your virthost

### On Your Virthost
- **Vagrant** 2.4+
- **libvirt** and **vagrant-libvirt** plugin
- **KVM/QEMU** virtualization enabled
- Sufficient resources:
  - 16GB+ RAM (8GB per VM)
  - 40GB+ disk space
  - 4+ CPU cores

## Quick Start

### Step 1: Configure Inventory

Copy and customize the virthost inventory:

```bash
cp inventory/virthost.yml.example inventory/virthost.yml
# Edit inventory/virthost.yml - update ansible_host with your virthost IP
```

The K8s cluster inventory (`inventory/k8s-cluster.yml`) is auto-generated by the vagrant-vm role, so you don't need to create it manually.

### Step 2: Create VMs

```bash
ansible-playbook playbooks/01-create-vms.yml -i inventory/virthost.yml
```

This creates 2 CentOS Stream 9 VMs using Vagrant/libvirt and generates the cluster inventory file.

### Step 3: Setup Kubernetes Cluster

```bash
ansible-playbook playbooks/02-setup-k8s-cluster.yml -i inventory/k8s-cluster.yml
```

This installs:
- Python 3.12 (required for Ansible modules)
- Kubernetes 1.35 via kubeadm
- CRI-O container runtime
- Calico CNI
- kubectl and Helm

### Step 4: Deploy Demo Assets

```bash
ansible-playbook playbooks/03-deploy-demo-assets.yml -i inventory/k8s-cluster.yml
```

This deploys:
- CDI (Container Device Interface) spec files for nvidia.com/gpu=all
- canhazgpu service (fake GPU simulator)
- fake-gpu-operator Helm chart
- Test pod manifests

### Step 5: Install K8s GPU Controller

```bash
ansible-playbook playbooks/04-install-k8shazgpu.yml -i inventory/k8s-cluster.yml
```

This installs the k8shazgpu DRA resource controller for GPU management.

## Verification

After all playbooks complete, verify the setup:

```bash
# SSH to control plane node
ssh dev@<centos9-vm1-ip>

# Check cluster status
kubectl get nodes
kubectl get pods -A

# Verify DRA ResourceClass
kubectl get resourceclass

# Check fake GPUs are detected
kubectl get nodes -o json | jq '.items[].status.allocatable'

# Deploy test pod with DRA GPU claim
kubectl apply -f /tmp/dra-h100-test-pod.yaml
kubectl get pods -w
```

## Customization

### Changing VM Resources

Edit `playbooks/roles/vagrant-vm/defaults/main.yml`:
- `vm_memory` - RAM per VM (default: 8192 MB)
- `vm_cpus` - CPU cores per VM (default: 4)
- `vm_count` - Number of VMs (default: 2)

### Changing Kubernetes Version

Edit `playbooks/roles/kubeadm/kube_init/defaults/main.yml`:
- `kube_version` - Kubernetes version (must be 1.35+ for DRA)

### Changing GPU Simulation

Edit `playbooks/files/dra-h100-values.yaml`:
- `replicaCount` - Number of simulated GPUs per node
- `gpu.model` - GPU model to simulate (e.g., H100, A100)

## Troubleshooting

### VMs Won't Start
- Check libvirt is running: `systemctl status libvirtd`
- Verify KVM enabled: `lsmod | grep kvm`
- Check Vagrant version: `vagrant --version` (need 2.4+)

### Kubernetes Init Fails
- Ensure VMs have internet access for package downloads
- Check CRI-O is running: `systemctl status crio`
- Verify swap is disabled: `swapon -s` (should be empty)

### DRA ResourceClass Not Found
- Kubernetes 1.35+ is required for DRA support
- Check k8shazgpu controller is running: `kubectl get pods -n kube-system | grep k8shazgpu`

### Fake GPUs Not Appearing
- Check canhazgpu service: `systemctl status canhazgpu-web`
- Verify fake-gpu-operator pods: `kubectl get pods -n fake-gpu-operator`
- Check node labels: `kubectl get nodes --show-labels | grep gpu`

### Pods Failing with CDI Device Injection Error
- If pods fail with "CDI device injection failed: unresolvable CDI devices nvidia.com/gpu=all":
  - Verify CDI spec exists: `ls -la /var/run/cdi/nvidia-gpu.yaml` on each node
  - Check CDI spec content: `cat /var/run/cdi/nvidia-gpu.yaml`
  - Re-run playbook 03 if CDI spec is missing (it's created in tmpfs and lost on reboot)

## Project Structure

```
.
├── ansible.cfg                      # Ansible configuration
├── inventory/
│   ├── virthost.yml.example         # Template for virthost inventory
│   └── k8s-cluster.yml              # Auto-generated by vagrant-vm role
├── playbooks/
│   ├── 01-create-vms.yml            # Create VMs with Vagrant
│   ├── 02-setup-k8s-cluster.yml     # Install Kubernetes
│   ├── 03-deploy-demo-assets.yml    # Deploy fake GPUs
│   ├── 04-install-k8shazgpu.yml     # Install DRA controller
│   ├── files/
│   │   ├── dra-h100-values.yaml     # Helm values for fake-gpu-operator
│   │   └── dra-h100-test-pod.yaml   # Test pod with DRA claim
│   └── roles/
│       ├── vagrant-vm/              # VM creation
│       ├── update_packages_centos/  # Package updates
│       ├── centos_python_upgrade/   # Python 3.12 upgrade
│       ├── kubeadm/                 # Kubernetes cluster setup
│       ├── kubernetes_tools/        # kubectl, Helm installation
│       ├── canhazgpu/               # Fake GPU service
│       └── k8shazgpu/               # DRA resource controller
└── templates/
    └── canhazgpu-web.service.j2     # Systemd service for GPU simulation
```

## Contributing

This is a demo/lab environment. For production DRA setups, consult the official Kubernetes documentation and your GPU vendor's resource drivers.

## License

MIT - Use freely for demos, labs, and learning.

## Credits

Created for KubeCon demos by [@dougbtv](https://github.com/dougbtv).
