---
# Playbook to set up a 2-node Kubernetes cluster on CentOS 9 Stream VMs
#
# Prerequisites:
#   - VMs created via 01-create-vms.yml
#   - inventory/k8s-cluster.yml generated by vagrant-vm role
#
# Usage:
#   ansible-playbook -i inventory/k8s-cluster.yml playbooks/02-setup-k8s-cluster.yml
#
# This playbook will:
#   - Update packages and upgrade Python to 3.12
#   - Install Kubernetes 1.35 with DRA support
#   - Install CRI-O container runtime
#   - Configure Flannel CNI
#   - Join worker nodes to the cluster
#   - Install kubectl, Helm, and canhazgpu service
#

# First play: Update and prepare all nodes
- name: Prepare all K8s cluster nodes
  hosts: k8s_cluster
  become: true
  vars:
    # Package update settings
    package_update_centos_force_reboot: false  # Don't reboot during initial setup

    # Set kube_version for DRA support
    kube_version: "v1.35"
    kubernetes_version: "1.35.0-0"
    enable_dra: true

  tasks:
    - name: Disable firewalld for dev cluster
      systemd:
        name: firewalld
        state: stopped
        enabled: false
      ignore_errors: true

    - name: Clean up DNF cache to fix repo issues
      shell: dnf clean all
      become: true

  roles:
    - update_packages_centos        # Updates all packages
    - centos_python_upgrade
    - kubeadm/prerequisites         # Install containerd/crio, kubeadm, kubectl, kubelet

# Second play: Initialize control plane
- name: Initialize Kubernetes control plane
  hosts: k8s_control_plane
  become: true
  vars:
    kube_version: "v1.35"
    kubernetes_version: "1.35.0-0"
    enable_dra: true
    kubeadm_advertise_address: "{{ ansible_host }}"

  roles:
    - kubeadm/kube_init            # Initialize Kubernetes cluster
    - kubeadm/kubectl_configure    # Configure kubectl for the user

  tasks:
    - name: Get kubeadm join command
      shell: kubeadm token create --print-join-command
      register: join_command

    - name: Save join command as fact
      set_fact:
        kubeadm_join_command: "{{ join_command.stdout }}"
      delegate_to: "{{ item }}"
      delegate_facts: true
      loop: "{{ groups['k8s_workers'] }}"

# Third play: Configure control plane kubectl and cluster post-setup
- name: Configure kubectl and install CNI on control plane
  hosts: k8s_control_plane
  become: false
  roles:
    - kubeadm/kube_niceties        # Add kubectl bash completion

  tasks:
    - name: Copy the kubeadm config file from /etc/kubernetes/admin.conf
      copy:
        src: /etc/kubernetes/admin.conf
        dest: /home/{{ ansible_user }}/.kube/admin.conf
        remote_src: yes
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        mode: '0644'
      become: true

    - name: Install CNI plugin (Flannel)
      shell: kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
      environment:
        KUBECONFIG: "/home/{{ ansible_user }}/.kube/admin.conf"

    - name: Wait for Flannel to write CNI config
      wait_for:
        path: /etc/cni/net.d/10-flannel.conflist
        timeout: 600
      become: true

    - name: Restart CRI-O to pick up CNI config
      systemd:
        name: crio
        state: restarted
      become: true

    - name: Restart kubelet to pick up CNI config
      systemd:
        name: kubelet
        state: restarted
      become: true

    - name: Wait for control plane node to be ready
      shell: kubectl get nodes --no-headers | grep -v Ready | wc -l
      environment:
        KUBECONFIG: "/home/{{ ansible_user }}/.kube/admin.conf"
      register: not_ready_nodes
      until: not_ready_nodes.stdout == "0"
      retries: 30
      delay: 10

    - name: Remove control-plane taint to allow workload scheduling
      shell: kubectl taint nodes --all node-role.kubernetes.io/control-plane:NoSchedule-
      environment:
        KUBECONFIG: "/home/{{ ansible_user }}/.kube/admin.conf"
      ignore_errors: true

# Fourth play: Join worker nodes to cluster
- name: Join worker nodes to cluster
  hosts: k8s_workers
  become: true

  tasks:
    - name: Check if node is already in cluster
      stat:
        path: /etc/kubernetes/kubelet.conf
      register: kubelet_conf

    - name: Join worker to cluster block
      block:
        - name: Reset kubeadm if previous join failed
          command: kubeadm reset -f
          ignore_errors: true

        - name: Join worker node to cluster
          shell: "{{ hostvars[inventory_hostname]['kubeadm_join_command'] }}"

        - name: Wait for Flannel to write CNI config on worker
          wait_for:
            path: /etc/cni/net.d/10-flannel.conflist
            timeout: 600

        - name: Restart CRI-O on worker to pick up CNI config
          systemd:
            name: crio
            state: restarted

        - name: Restart kubelet on worker to pick up CNI config
          systemd:
            name: kubelet
            state: restarted
      when: not kubelet_conf.stat.exists

# Fifth play: Verify cluster status
- name: Verify cluster is ready
  hosts: k8s_control_plane
  become: false

  tasks:
    - name: Wait for all nodes to be ready
      shell: kubectl get nodes --no-headers | grep -v Ready | wc -l
      environment:
        KUBECONFIG: "/home/{{ ansible_user }}/.kube/admin.conf"
      register: not_ready_nodes
      until: not_ready_nodes.stdout == "0"
      retries: 60
      delay: 10

    - name: Get cluster status
      shell: kubectl get nodes -o wide
      environment:
        KUBECONFIG: "/home/{{ ansible_user }}/.kube/admin.conf"
      register: cluster_status

    - name: Get pod status
      shell: kubectl get pods -A
      environment:
        KUBECONFIG: "/home/{{ ansible_user }}/.kube/admin.conf"
      register: pod_status

    - name: Display cluster information
      debug:
        msg: |
          ============================================
          Kubernetes 2-Node Cluster is Ready!
          ============================================

          Cluster Status:
          {{ cluster_status.stdout }}

          Pod Status:
          {{ pod_status.stdout }}

          To access your cluster from the control plane:
          export KUBECONFIG=/home/{{ ansible_user }}/.kube/admin.conf
          kubectl get nodes
          kubectl get pods -A

          CNI: Flannel
          Kubernetes Version: 1.35.0 (DRA enabled)
          Control Plane: {{ groups['k8s_control_plane'] | join(', ') }}
          Workers: {{ groups['k8s_workers'] | join(', ') }}

          Next step: Run playbooks/03-deploy-demo-assets.yml

- name: Install k8s tools
  hosts: k8s_control_plane
  become: true
  roles:
    - kubernetes_tools

- name: Install canhazgpu
  hosts: k8s_cluster
  become: true
  vars:
    canhazgpu_version: "v0.1.0-k8shazgpu"
    canhazgpu_release_url: "https://github.com/dougbtv/canhazgpu/releases/download/{{ canhazgpu_version }}/canhazgpu_Linux_x86_64.tar.gz"
    canhazgpu_port: 80
    canhazgpu_num_gpus: 8
    canhascheezeburger: plz
    canhazgpu_provider: virtual
  roles:
    - canhazgpu
